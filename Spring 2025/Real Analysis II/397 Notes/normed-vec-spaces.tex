\chapter{Vector Spaces, Algebras, and Normed Spaces}
\pagenumbering{arabic}

\noindent For the entirety of this chapter assume $F$ to be  $\bfR$ or $\bfC$.

\section{Vector Spaces}
    \begin{definition}
        A \textit{vector space} (or \textit{linear space}) over $F$ is a nonempty set $V$ equipped with two operations:
            \begin{equation*}
            \begin{split}
                V \times V \xrightarrow{+} V &\mtext{defined by} (v,w) \mapsto v+w \\
                F \times V \rightarrow V &\mtext{defined by} (\alpha,v) \mapsto \alpha v
            \end{split}
            \end{equation*}
        satisfying:
            \begin{enumerate}[label = (\arabic*),itemsep=1pt,topsep=3pt]
                \item $(V, +)$ is an abelian group:
                    \begin{enumerate}[label = (\roman*),itemsep=1pt,topsep=3pt]
                        \item $u + (v+w) = (u+v) + w$ for all $u,v,w \in V$;
                        \item there exists $0_V$ such that $v + 0_V = 0_V + v = v$ for all $v \in V$;
                        \item for all $v \in V$, there exists $w \in V$ satisfying $v+w = w+v = 0_V$;
                        \item $v + w = w+v$ for all $v,w \in V$;
                    \end{enumerate}

                \item $\alpha(v+w) = \alpha v + \alpha w$ for all $\alpha \in F$, $v,w \in V$;
                \item $\alpha(\beta v) = (\alpha \beta)v$ for all $\alpha,\beta \in F$, $v \in V$;
                \item $1_F v = v$ for all $v \in V$.
            \end{enumerate}
    \end{definition}

    It can be shown that the vector $0_V$ is unique, the additive inverse in (iii) is unique (which we denote as $-v$), that $0v = 0_V$, and $(-1)v = -v$.

    \begin{exercise}
        Show (iv) follows from the other axioms.
    \end{exercise}

    \begin{exercise}
        Show $nv = \underbrace{v + v + ... + v}_{n \hspace{-4pt}\mtext{times}}$ for $n \in \bfZ_{\geq 1}$.
    \end{exercise}

    It can be shown that a subspace is a vector space in its own right.

    \begin{example}
        Let $\{W_i\}_{i \in I}$ be a family of vector spaces. Then $\bigcap_{i \in I}W_i$ is also a vector space.
    \end{example}

    \begin{example}
        Planes and lines through the origin are subspaces of $\bfR^3$.
    \end{example}

    \begin{definition}
        Let $V$ be a vector space and $S \subseteq V$ a subset.
            \begin{enumerate}[label = (\arabic*),itemsep=1pt,topsep=3pt]
                \item A \textit{linear combination} from $S$ is a finite sum $\sum_{j = 1}^n \alpha_j v_j$ with $\alpha_j \in F$, $v_j \in S$.
                \item The \textit{linear span} of $S$ is:
                    \begin{equation*}
                    \begin{split}
                        \Span(S) := \left\{ \sum_{j = 1}^n \alpha_j v_j \hspace{4pt}\middle|\hspace{4pt} n \in \bfN, \alpha_j \in F, v_j \in S \right\}.
                    \end{split}
                    \end{equation*}
            \end{enumerate}
    \end{definition}

    \begin{exercise}
        Show that $\Span(S) \subseteq V$ is a subspace and:
            \begin{equation*}
            \begin{split}
                \Span(S) = \bigcap \hspace{2pt}\{W \mid S \subseteq W, W \hspace{-2pt}\mtext{is a subspace}\hspace{-4pt}\},
            \end{split}
            \end{equation*}
        that is, $\Span(S)$ is the smallest subspace of $V$ containing $S$.
    \end{exercise}

    \begin{definition}
        Let $V$ be a vector space and $S \subseteq V$ a subset.
            \begin{enumerate}[label = (\arabic*),itemsep=1pt,topsep=3pt]
                \item $S$ is \textit{spanning} for $V$ if $\Span(S) = V$.
                \item $S$ is \textit{independent} if, given $n \in \bfN$, $\alpha_1,...,\alpha_n \in F$, $v_1,...,v_n \in S$, then $\sum_{j = 1}^n \alpha_j v_j = 0$ implies $\alpha_j = 0$ for all $j$.
            \end{enumerate}
    \end{definition}

    \begin{center}
        \begin{tikzpicture}
            \draw[thick] (0.3,0) -- (2.3,0);
            \node at (2.39, 0) {$/\,$};
            \node at (2.56, 0) {$/\,$};
            \draw[thick] (2.6,0) -- (4.6,0);
        \end{tikzpicture}
    \end{center}

    Our goal is to show that every vector space admits a basis. As such, recall the following definitions from a standard course in Real Analysis.

    \begin{definition}
        An \textit{ordering} on a set $X$ is a relation $R \subseteq X \times X$ on $X$ that is reflexive, transitive, and antisymmetric. We write $xRy$ as $x \leq_R y$. The pair $(X,\leq_R)$ is called an \textit{ordered set}. An ordering $\leq$ on $X$ is called \textit{total} (or \textit{linear}) if for all $x,y \in X$, $x \leq y$ or $y \leq x$.
    \end{definition}

    Note that if $(X,\leq)$ is an ordered set and $Y \subseteq X$ is a subset, then $(Y,\leq)$ is an ordered set as well.

    \begin{definition}
        Let $(X, \leq)$ be an ordered set and $Y \subseteq X$. An \textit{upper bound} for $Y$ is an element $u \in X$ with $u \geq y$ for all $y \in Y$. An element $m \in X$ is called \textit{maximal} if $x \in X$, $x \geq m$ implies $x = m$.
    \end{definition}

    \begin{lemma}[Zorn's Lemma]
        Let $(X,\leq_X)$ be an ordered set. Suppose every subset $Y \subseteq X$ for which $(Y,\leq_X)$ is totally ordered has an upper bound in $X$. Then $X$ admits a maximal element.
    \end{lemma}

    The proof of Zorn's Lemma is outside the interest of this text. 

    \begin{theorem}
        Every vector space admits a basis. Moreover, every independent set is contained in a basis.
    \end{theorem}
        \begin{proof}
            Let $S \subseteq V$ be linearly independent. Define:
                \begin{equation*}
                \begin{split}
                    \fT(S) = \{T \subseteq V \mid S \subseteq T, T \hspace{-3pt}\mtext{linearly independent}\hspace{-5pt}\}.
                \end{split}
                \end{equation*}
            Let $\fC \subseteq \fT(S)$ be a totally ordered subset. Set $R = \bigcup_{T \in \fC} T$. Clearly $R \supseteq S$. Assume $\sum_{j = 1}^n \alpha_j v_j = 0$, where $\alpha_j \in F$ and $v_j \in R$. Since $\fC$ is totally ordered, there exists $T_0 \in \fC$ with $v_j \in T_0$ for all $j = 1,...,n$. Since $T_0$ is independent, $\alpha_j = 0$ for all $j = 1,...,n$. Thus $R$ is independent as well. Whence $R$ is an upper bound for $\fC$. By Zorn's Lemma, $\fT(S)$ admits a maximal element, call it $B$.

            Claim: $B$ is a basis for $V$. Suppose towards contradiction it's not, then there exists $v_0 \in V \setminus \Span(B)$. Consider $B \cup \{v_0\}$ and let $\alpha_0 v_0 + \sum_{j = 1}^n \alpha_j v_j = 0_V$. If $\alpha_0 \neq 0$, then $\sum_{j = 1}^n \alpha_j v_j = -\alpha_0 v_0 $, giving $v_0 \in \Span(B)$ which is a contradiction. If $\alpha_0 = 0$, then $\sum_{j = 1}^n \alpha_j v_j = 0_V$. Since $B$ is independent, $\alpha_j = 0$ for all $j =1,...,n$. Thus $B \cup \{v_0\}$ is independent, contradicting the maximality of $B$. Whence $B$ is a basis for $V$.
        \end{proof}

    \begin{theorem}
        If $B_1$ and $B_2$ are bases for $V$, then $\card(B_1) = \card(B_2)$.
    \end{theorem}

    \begin{definition}
        If $V$ is a vector space, its \textit{dimension} is the cardinality of any of its bases.
    \end{definition}

    \begin{corollary}
        If $B$ is a basis for $V$, then every $v \in V$ can be written $v = \sum_{j = 1}^n \alpha_k \beta_k$, $\alpha_k \in F$, $b_k \in B$ in a unique way.
    \end{corollary}

    \begin{theorem}
        Let $V$ be a linear space and $B \subseteq V$ a subset. The following are equivalent:
            \begin{enumerate}[label = (\arabic*),itemsep=1pt,topsep=3pt]
                \item $B$ is a basis for $V$;
                \item $B$ is a maximal element in $\fT = \{T \subseteq V \mid \hspace{-4pt}\mtext{$T$ independent}\hspace{-4pt}\}$;
                \item $B$ is a minimal element in $\fS = \{S \subseteq V \mid \hspace{-4pt}\mtext{$S$ spans $V$}\hspace{-4pt}\}$;
            \end{enumerate}
    \end{theorem}

    \begin{definition}
        Let $\{V_i\}_{i \in I}$ be a family of vector spaces over a field $F$.
            \begin{enumerate}[label = (\arabic*),itemsep=1pt,topsep=3pt]
                \item The \textit{product} of $\{V_i\}_{i \in I}$ is denoted:
                    \begin{equation*}
                    \begin{split}
                        \prod_{i \in I}V_i := \{(v_i)_{i \in I} \mid v_i \in V_i\}.
                    \end{split}
                    \end{equation*}
                \item The \textit{co-product} (or \textit{sum}) is denoted 
                    \begin{equation*}
                    \begin{split}
                        \bigoplus_{i \in I}V_i := \left\{(v_i)_{i \in I} \mid v_i \in V_i,\hspace{2pt} \supp\bigl((v_i)_{i \in I}\bigr) <\infty \right\}.
                    \end{split}
                    \end{equation*}
            \end{enumerate}
    \end{definition}

    \newpage
    \begin{exercise}
        \phantom{a}
        \begin{enumerate}[label = (\arabic*),itemsep=1pt,topsep=3pt]
            \item Show that $\prod_{i \in I}V_i$ equipped with pointwise operations:
                \begin{equation*}
                \begin{split}
                    (v_i)_{i \in I} + (w_i)_{i \in I} &= (v_i + w_i)_{i \in I} \\
                    \alpha(v_i)_{i \in I} &= (\alpha v_i)_{i \in I}
                \end{split}
                \end{equation*}
            is a linear space.

            \item Show that $\bigoplus_{i \in I}V_i$ is a subspace of $\prod_{i \in I}V_i$.
        \end{enumerate}
    \end{exercise}

    \begin{proposition}
        Let $V$ be a vector space over $F$ and $W \subseteq V$. The (additive, abelian) quotient group $V/W$ can be made into a vector space by defining multiplication by scalars as $\alpha(v + W) = \alpha v + W$ for all $\alpha \in F$, $v + W \in V/W$.
    \end{proposition}

    \begin{example}
        \phantom{a}
        \begin{enumerate}[label = (\arabic*),itemsep=1pt,topsep=3pt]
            \item The set $F^n = \{(x_1,...,x_n) \mid x_j \in F\}$ with component-wise operations is a vector space.
            \item The set $M_{n,m}(F) = \{(a_{ij}) \mid a_{ij} \in F\}$ with linear operations is a vector space.
            \item Let $\Omega$ be a nonempty set. Then $\cF(\Omega,F) = \{f \mid f:\Omega \rightarrow F\}$ with pointwise operations is a vector space.
            \item The set $\ell_\infty(\Omega,F) = \{f \in \cF(\Omega,F) \mid \lnorm f \rnorm _u < \infty \}$ with pointwise operations is a vector space.
                \begin{exercise}
                    Show $\ell_\infty(\Omega,F) \subseteq \cF(\Omega,F)$ is a subspace.
                \end{exercise}
            \item Let $K \subseteq V$ be a convex subset of a vector space $V$, that is, for all $v,w \in K$ and $t \in [0,1]$, then $(1-t)v + tw \in K$. A function $f:K \rightarrow F$ is said to be \textit{affine} if $x,y \in K$ and $t \in [0,1]$ implies $f((1-t)x + ty) = (1-t)f(x) + tf(y)$. The set \newline $\text{Aff}(K,F) = \{f \in \cF(\Omega,F) \mid f \hspace{3pt}\text{affine}\hspace{1pt}\}$ with pointwise operations is a vector space.
                \begin{exercise}
                    Show $\text{Aff}(\Omega,F) \subseteq \cF(\Omega,F)$ is a subspace.
                \end{exercise}
            \item The set $C([a,b],F) = \{f:[a,b] \rightarrow F \mid f \hspace{3pt}\text{continuous}\hspace{1pt}\}$ with pointwise operations is a vector space.
                \begin{exercise}
                    Explain why $C([a,b],F) \subseteq \ell_\infty([a,b],F)$ is a subspace.
                \end{exercise}
            \item Consider the following sequence spaces:
                \begin{itemize}
                    \item $s = \{(a_k)_k \mid a_k \in F\} = \cF(\bfN,F)$;
                    \item $\ell_\infty = \ell_\infty(\bfN,F) = \{(a_k)_k \mid \sup_{k \geq 1}|a_k| < \infty\}$;
                    \item $c = \{(a_k)_k \mid (a_k)_k \hspace{3pt}\text{converges}\hspace{2pt}\}$;
                    \item $c_0 = \{(a_k)_k \mid (a_k)_k \rightarrow 0\}$;
                    \item $c_{00} = \{(a_k)_k \mid \supp(a_k)_k < \infty\}$;
                    \item $\ell_1 = \left\{ (a_k)_k \mid \sum_{k = 1}^\infty |a_k| \hspace{3pt}\text{converges}\hspace{2pt} \right\}$.
                \end{itemize}
            These are all vector spaces with pointwise operations. In fact, $c_{00} \subseteq c_0 \subseteq c \subseteq \ell_\infty \subseteq s$ are all subspaces.
                \begin{exercise}
                    Show that $\ell_1 \subseteq c_0$ is a subspace.
                \end{exercise}

            \item Consider the following continuous function spaces on $\bfR$:
                \begin{itemize}
                    \item $C(\bfR) = \{f:\bfR \rightarrow F \mid f \hspace{3pt}\text{continuous}\hspace{2pt}\}$;
                    \item $C_b(\bfR) = C(\bfR) \cap \ell_\infty(\bfR)$;
                    \item $C_0(\bfR) = \{f \in C(\bfR) \mid \limit_{x \rightarrow \pm\infty} f(x) = 0\}$;
                    \item Recall that a function is \textit{compactly supported} if for all $\epsilon > 0$, there exists $\alpha > 0$ such that $|x| \geq \alpha$ implies $f(x) = 0$. The set of compactly supported functions is denoted $C_c(\bfR) = \{f \in C(\bfR) \mid f \hspace{3pt}\text{compactly supported}\hspace{2pt}\}$.
                \end{itemize}
            These are all vector spaces with pointwise operations, and $C_c(\bfR) \subseteq C_0(\bfR) \subseteq C_b(\bfR) \subseteq C(\bfR)$ are all subspace inclusion.
        \end{enumerate}
    \end{example}

    \begin{definition}
        If $V$ and $W$ are linear spaces over a common field $F$, a map $T:V \rightarrow W$ is called \textit{linear} if $T(v_1 + \alpha v_2) = T(v_1) + \alpha T(v_2)$ for all $v_1,v_2 \in V$ and $\alpha \in F$.
    \end{definition}
    
    \begin{example}
        Let $A \in M_{m,n}(F)$. Then $T_A:F^n \rightarrow F^m$ defined by $T_A(v) = Av$ is linear. Let $\{e_1,...,e_n\}$ be a basis for $F^n$. If $T:F^n \rightarrow F^m$ is linear, set:
            \begin{equation*}
            \begin{split}
                [T] = \Bigl( T(e_1) \Bigm\vert T(e_2) \Bigm\vert \hspace{4.5pt}...\hspace{4.5pt}\Bigm\vert T(e_n) \Bigr).
            \end{split}
            \end{equation*}
        This gives $T(v) = [T]v$ for all $v \in F^n$. In fact, we also have $[T_A] = A$ and $T_{[T]} = T$.
    \end{example}

    \begin{example}
        The \textit{canonical projection} is linear:
            \begin{equation*}
            \begin{split}
                \pi_j: \prod_{i \in I}V_i \rightarrow V_j \mtext{defined by} \pi_j\bigl((v_i)_i\bigr) = v_i.
            \end{split}
            \end{equation*}
        We also have that the \textit{coordinate exclusions} are linear:
            \begin{equation*}
            \begin{split}
                \iota_j: V_j \hookrightarrow \bigoplus_{i \in I}V_i \mtext{defined by} \iota_j(v) = (v_i)_i \hspace{1.5pt}, \hspace{-2pt}\mtext{where} v_i = \begin{cases}
                    0_v, &i \neq j \\
                    v_j, &\text{otherwise}.
                \end{cases}
            \end{split}
            \end{equation*}
        The \textit{evaluation map} is linear as well. For $s \in S$, consider:
            \begin{equation*}
            \begin{split}
                e_s : \cF(S,F) \rightarrow F \mtext{defined by} e_s(f) = f(s).
            \end{split}
            \end{equation*}
    \end{example}

    \begin{proposition}*
        Let $V$ be a vector space with basis $B$. Let $W$ be a vector space and suppose $\varphi:B \rightarrow W$ is a map. Then there exists a unique linear map $T_\varphi: V \rightarrow W$ with $T_\varphi(b) = \varphi(b)$ for all $b \in B$.
    \end{proposition}
        \begin{proof}
            
        \end{proof}

    \begin{proposition}*
        Let $T:V \rightarrow W$ be linear.
            \begin{enumerate}[label = (\arabic*),itemsep=1pt,topsep=3pt]
                \item $\ker(T) = \{v \in V \mid T(v) = 0_W\}$ is a linear subspace of $V$.
                \item $\Image(T) = \{T(v) \mid v \in V\}$ is a linear subspace of $W$.
                \item $\ker(T) = \{0_V\}$ if and only if $T$ is injective.
                \item $\Image(T) = W$ if and only if $W$ is surjective.
            \end{enumerate}
    \end{proposition}
        \begin{proof}
            (1) Let $v_1,v_2 \in \ker(T)$ and $\alpha \in F$. Observe that:
                \begin{equation*}
                \begin{split}
                    T(v_1 + cv_2)
                    & = T(v_1) + cT(v_2) \\
                    & = 0.
                \end{split}
                \end{equation*}
            Thus $v_1 + cv_2 \in \ker(T)$, giving $\ker(T)$ as a linear subspace of $V$.

            (2) Let $w_1,w_2 \in \Image(T)$. Then there exists $v_1.v_2 \in V$ with $T(v_1) = w_1$ and $T(v_2) = w_2$. We have:
                \begin{equation*}
                \begin{split}
                    w_1 + cw_2 
                    & = T(v_1) + cT(v_2) \\
                    & = T(v_1 + cv_2).
                \end{split}
                \end{equation*}
            Whence $w_1 + cw_2 \in \Image(T)$, giving $\Image(T)$ as a linear subspace of $W$.

            (3) Let $\ker(T) = \{0\}$. Suppose $T(v_1) = T(v_2)$. Then $T(v_1) - T(v_2) = T(v_1 - v_2) = 0_W$. It must be that $v_1 -v_2 = 0_W$, giving $v_1 = v_2$. Thus $T$ is injective. Conversely, suppose $T$ is injective and let $v \in \ker(T)$. Then $T(v) = 0_W = T(0_V)$. Hence $v = 0_V$, establishing $\ker(T) = \{0\}$.

            (4)
        \end{proof}

    \begin{proposition}
        If $T:V \rightarrow W$ is linear and bijective, then the inverse map $T^{-1}:W \rightarrow V$ is linear.
    \end{proposition}
        \begin{proof}
            We have that:
                \begin{equation*}
                \begin{split}
                    T(T^{-1}(w_1) + \alpha T^{-1}(w_2)) = w_1 + \alpha w_2 = T \circ T^{-1}(w_1 + \alpha w_2).
                \end{split}
                \end{equation*}
            Applying $T^{-1}$ to both sides gives the desired result.
        \end{proof}

    \begin{proposition}[Vector Spaces are Injective]*
        Let $U,V,W$ be vector spaces and $0 \rightarrow U \xrightarrow{j} V$ be exact (that is, $j$ is injective). Let $\varphi:U \rightarrow W$ be linear. There exists a linear map $\psi:V \rightarrow W$ such that $\varphi = \psi\circ j $; i.e., the following diagram commutes:
            \begin{center}
                \begin{tikzcd}
                    0 \arrow[r] & U \arrow[r, "j"] \arrow[d, "\varphi"'] & V \arrow[ld, "\psi", dotted] \\
                                & W                                      &                             
                    \end{tikzcd}
            \end{center}
    \end{proposition}
        \begin{proof}
            Let $\{u_i\}_{i \in I}$ be a basis for $U$. Claim: $\{j(u_i)\}_{i \in I}$ is linearly independent. Observe that:
                \begin{equation*}
                \begin{split}
                    0 
                    & = \sum_{i \in I}\alpha_i j(u_i) \\
                    & = j \left( \sum_{i \in I}\alpha_i u_i \right).
                \end{split}
                \end{equation*}
            By the injectivivity of $j$, we have that $\sum_{i = 1}^n \alpha_i u_i = 0$, giving $\alpha_i = 0$ for all $i \in I$. Thus $\{j(u_i)\}_{i \in I}$ is linearly independent. We can extend this set to a basis of $V$ as follows: let 
        \end{proof}

    \begin{proposition}[Vector Spaces are Projective]*
        Let $U,V,W$ be vector spaces and $V \xrightarrow{\pi}U \rightarrow 0$ be exact (that is, $\pi$ is onto). Let $\varphi:W \rightarrow U$ be linear. There exists a linear map $\phi:V \rightarrow W$ such that $\varphi = \pi \circ \psi$; i.e., the following diagram commutes:
            \begin{center}
                \begin{tikzcd}
                    & W \arrow[d, "\varphi"] \arrow[ld, "\psi"', dotted] &   \\
                    V \arrow[r, "\pi"] & U \arrow[r]                                        & 0
                \end{tikzcd}
            \end{center}
    \end{proposition}
        \begin{proof}
            
        \end{proof}

    \begin{definition}
        Let $V$ and $W$ be vector spaces over $F$. A \textit{linear isomorphism} between $V$ and $W$ is a bijective linear map $T:V \rightarrow W$. If such a $T$ exists, we say $V$ and $W$ are \textit{linearly isomorphic}, and write $V \cong W$.
    \end{definition}

    \begin{center}
        \begin{tikzpicture}
            \draw[thick] (0.3,0) -- (2.3,0);
            \node at (2.39, 0) {$/\,$};
            \node at (2.56, 0) {$/\,$};
            \draw[thick] (2.6,0) -- (4.6,0);
        \end{tikzpicture}
    \end{center}

    Finite dimensional vector spaces are boring. This is illustrated through the following theorem.

    \begin{theorem}
        Let $V$ and $W$ be finite-dimensional vector spaces over $F$. Then $V \cong W$ if and only if $\dim(V) = \dim(W)$.
    \end{theorem}
        \begin{proof}
            Suppose $V \cong W$. Then there is an isomorphism taking basis of $V$ to a basis of $W$. Therefore they have the same dimension.

            Conversely, if $\dim(V) = \dim(W) = n$, then they are each isomorphic to $F^n$, giving that they are isomorphic to eachother.
        \end{proof}

        \begin{center}
            \begin{tikzpicture}
                \draw[thick] (0.3,0) -- (2.3,0);
                \node at (2.39, 0) {$/\,$};
                \node at (2.56, 0) {$/\,$};
                \draw[thick] (2.6,0) -- (4.6,0);
            \end{tikzpicture}
        \end{center}

    \begin{example}
        Let $V$ be a vector space, $W \subseteq V$ a subspace. The \textit{natural projection}:
            \begin{equation*}
            \begin{split}
                \pi:V \rightarrow V/W \mtext{defined by}\pi(v) = v+W
            \end{split}
            \end{equation*}
        is a linear surjective map.
    \end{example}

    \begin{theorem}[First Isomorphism Theorem for Vector Spaces]*
        Let $T:V \rightarrow V'$ be a linear map and $W \subseteq V$ a subspace.
        \begin{enumerate}[label = (\arabic*),itemsep=1pt,topsep=3pt]
            \item If $T$ "kills" $W$ (that is, $W \subseteq \ker(T)$), then there exists a linear map $\widetilde{T}:V/W \rightarrow V'$ with $\widetilde{T} \circ \pi = T$; i.e., the following diagram commutes.
                \begin{center}
                    \begin{tikzcd}
                        V \arrow[rr, "T"] \arrow[rd, "\pi"'] &                                & V' \\
                        & V/W \arrow[ru, "\widetilde{T}"', dotted] &   
                    \end{tikzcd}
                \end{center}

            \item If $\ker(T) = W$, then $\widetilde{T}$ is injective.
            \item If $\ker(T) = W$ and $\Image(T) = V'$, then $V/W \cong V'$.
        \end{enumerate}
    \end{theorem}
        \begin{proof}
            (1) As stipulated, define $\widetilde{T}(v+W) = T(v)$. We must show that $\widetilde{T}$ is well-defined: suppose $v_1 + W = v_2 + W$ for some $v_1,v_2 \in V$. Then $v_1 = v_2 + w$ for some $w \in W$. This gives:
                \begin{equation*}
                \begin{split}
                    \widetilde{T}(v_1 + W)
                    & = \widetilde{T}(v_2 + w + W) \\
                    & = \widetilde{T}(v_2 + W).
                \end{split}
                \end{equation*}
            Whence $\widetilde{T}$ is well-defined. Now given $v_1 + W, v_2 +W \in V/W$ and $\alpha \in F$, observe that:
                \begin{equation*}
                \begin{split}
                    \widetilde{T}\bigl((v_1 + W) + c (v_2 + W)\bigr)
                    & = \widetilde{T}\bigl( (v_1 + cv_2) + W\bigr) \\
                    & = T(v_1 + cv_2) \\
                    & = T(v_1) + cT(v_2) \\
                    & = \widetilde{T}(v_1 + W) + c \widetilde{T}(v_2 + W). 
                \end{split}
                \end{equation*}
            Thus $\widetilde{T}$ is linear.
        \end{proof}

    \begin{definition}
        Let $S$ be a nonempty set. The \textit{free vector space} of $S$ is:
            \begin{equation*}
            \begin{split}
                \bfF(S) = \{f:S \rightarrow F \mid \supp(f) <\infty \}.
            \end{split}
            \end{equation*}
    \end{definition}

    \begin{exercise}
        Show $\bfF(S) \subseteq \cF(S,F)$ is a subspace.
    \end{exercise}

    \begin{proposition}
        The set $\{\delta_s \mid s \in S\}$ is a basis for $\bfF(S)$, where $\delta_s :S \rightarrow F$ is defined by:
            \begin{equation*}
            \begin{split}
                \delta_s(t) = \begin{cases}1, & t =0 \\ 0, & \text{otherwise.} \end{cases}
            \end{split}
            \end{equation*}
    \end{proposition}
        \begin{proof}
            If $f \in \bfF(S)$ with $\supp(f) = \{s_1,...,s_n\}$, then $f = \sum_{k =1}^n f(s_k)\delta_{s_k}$. If \newline$\sum_{k = 1}^n \alpha_k \delta_{s_k} = 0$, then for $j = 1,...,n$ we have $0 = \left( \sum_{k  =1}^n \alpha_k \delta_{s_k} \right)(s_j) = \alpha_j$.
        \end{proof}

    \begin{theorem}*
        Given any vector space $V$ and a map (of sets) $\varphi:S \rightarrow V$, there exists a unique linear map $T_\varphi : \bfF(S) \rightarrow V$ with $T_\varphi \circ \iota = \varphi$, where $\iota:S \rightarrow \bfF(S)$ is defined by $\iota(s) = \delta_s$ for all $s \in S$. In other words, the following diagram commutes:
            \begin{center}
                \begin{tikzcd}
                    S \arrow[r, "\iota"] \arrow[r] \arrow[rd, "\varphi"'] & \bfF(S) \arrow[d, "T_\varphi", dotted] \\
                                                                          & V                                     
                    \end{tikzcd}
            \end{center}
    \end{theorem}
        \begin{proof}

        \end{proof}

    \begin{definition}
        Let $V$ and $W$ be vector spaces. The set of linear transformations between $V$ and $W$ is $\cL(V,W) = \{T \mid T:V \rightarrow W \hspace{3pt}\text{linear}\hspace{2pt}\}$. The set of linear functionals is $V' := \cL(V,F)$.
    \end{definition}

    \begin{exercise}
        Show $\cL(V,W)$ is a vector space.
    \end{exercise}
    
    \begin{exercise}
        Show $M_{m,n}(F) \cong \cL(F^m,F^n)$ by $a \mapsto T_a:(v \mapsto av)$.
    \end{exercise}

\section{Algebras}
    \begin{definition}
        An \textit{algebra} over $F$ is a linear space $A$ over $F$ equipped with a multiplication operation:
            \begin{equation*}
            \begin{split}
                A \times A \rightarrow A \mtext{defined by} (a,b) \mapsto ab
            \end{split}
            \end{equation*}
        satisfying:
            \begin{enumerate}[label = (\arabic*),itemsep=1pt,topsep=3pt]
                \item $(ab)c = a(bc)$ for all $a,b,c \in A$;
                \item $(\alpha a)b = \alpha(ab) = a (\alpha b)$ for all $a,b \in A$, $\alpha \in F$;
                \item $a(b+c) = ab + ac$ for all $a,b,c \in A$;
                \item $(a+b)c = ac + bc$ for all $a,b,c \in A$.
            \end{enumerate}
        If $ab=ba$ for all $a,b \in A$ we say that $A$ is \textit{commutative}. If there exists $1_A \in A$ with $1_A a = a 1_A = a$ for all $a \in A$ we say $A$ is \textit{unital}.
    \end{definition}

    \begin{example}
        \phantom{a}
        \begin{enumerate}[label = (\arabic*),itemsep=1pt,topsep=3pt]
            \item $M_n(F)$ is a noncommutative unital algebra over $F$ under the usual matrix multiplication.
            \item If $V$ is a vector space over $F$, $\cL(V)$ is a unital algebra over $F$. It is noncommutative provided $\dim(V) > 1$.
            \item $\cF(S,F)$ is a unital commutative algebra over $F$.
        \end{enumerate}
    \end{example}

    \begin{definition}
        Let $B$ be a (unital) algebra over $F$.
        \begin{enumerate}[label = (\arabic*),itemsep=1pt,topsep=3pt]
            \item A (unital) \textit{subalgebra} of $B$ is a subspace $A \subseteq B$ ($1_B \in A$) satisfying the property that if $a,a' \in A$, then $aa' \in A$.
            \item An \textit{ideal} of $B$ is a subspace $I \subseteq B$ with $b \in B$, $a \in I$ implying $ba,ab \in I$.
        \end{enumerate} 
    \end{definition}

    \begin{example}
        \phantom{a}
        \begin{enumerate}[label = (\arabic*),itemsep=1pt,topsep=3pt]
            \item $\ell_\infty(\Omega,F) \subseteq \cF(\Omega,F)$ is a unital subalgebra.
            \item $c_{00} \subseteq c_0 \subseteq c \subseteq \ell_\infty \subseteq s$ are all subalgebras. In particular, $c_0 \subseteq \ell_\infty$ and $c_{00} \subseteq s$ are ideals.
            \item $C\bigl([a,b]\bigr) \subseteq \ell_\infty\bigl([a,b]\bigr)$ is a unital subalgebra.
            \item $C_c(\bfR) \subseteq C_0(\bfR) \subseteq C_b(\bfR) \subseteq \ell_\infty(\bfR)$ are all subalgebras. In fact, $C_b(\bfR) \subseteq C(\bfR)$ and $C_b(\bfR) \subseteq \ell_\infty(\bfR)$ are unital, whereas $C_0(\bfR) \subseteq C_b(\bfR)$ and $C_c(\bfR) \subseteq C(\bfR)$ are ideals.
            \item The set $T_n(F) = \{(a_{ij}) \in M_n(F) \mid a_{ij} = 0, i> j\}$ is a unital subalgebra of $M_n(F)$.
        \end{enumerate}
    \end{example}

    \begin{example}[Group Algebra]
        Let $\Gamma$ denote a group (not necessarily abelian). Take the free vector space $\bfF(\Gamma)$ and define multiplication as \textit{convolution}: given $f,g \in \bfF(\Gamma)$ let:
            \begin{equation*}
            \begin{split}
                (f \ast g)(r) = \sum_{
                    \mathclap{\substack{\bigl\{ (s,t) \hspace{2pt} \mid \\ s \in \supp(f), \\ t \in \supp(g), \\ st = r \bigr\}}}} f(s)g(t).
            \end{split}
            \end{equation*}
        Since $\supp(f)$ and $\supp(g)$ are finite, this is a finite sum. We often suppress this notation and write $(f \ast g)(r) = \sum_{st = r}f(s)g(t)$.

        We can also make substitutions:
            \begin{equation*}
            \begin{split}
                (f \ast g)(r)
                & = \sum_{st = r}f(s)g(t) \\
                & = \sum_{t \in \Gamma} f(rt^{-1})g(t) \\
                &  =\sum_{s \in \Gamma}f(s)g(s^{-1}r).
            \end{split}
            \end{equation*}
        It is clear that:
            \begin{equation*}
            \begin{split}
                (f+g)\ast h &= f \ast h + g \ast h \\\
                g \ast (g+h) &= f \ast g + f \ast h \\
                \alpha(f \ast g) &= (\alpha f)\ast g = f \ast (\alpha g)
            \end{split}
            \end{equation*}
        for $f,g,h \in \bfF(\Gamma)$, $\alpha \in F$. Associativity can be similarly shown using the above definition. Rather, we will prove associativity by first show that $\delta_s \ast \delta_t = \delta_{st}$. Given:
            \begin{equation*}
            \begin{split}
                (\delta_s \ast \delta_t)(r) = \sum_{q \in \Gamma} \delta_s(rq^{-1})\delta_t(q),
            \end{split}
            \end{equation*}
        notice that:
            \begin{equation*}
            \begin{split}
                \delta_s(rt^{-1}) = \begin{cases}
                    1, &s = rt^{-1} \\
                    0, &\text{otherwise}
                \end{cases}
                \hspace{5pt}=\hspace{5pt}
                \begin{cases}
                    1,& r = st \\
                    0 ,& \text{otherwise}
                \end{cases}
                \hspace{5pt} = \delta_{st}(r).
            \end{split}
            \end{equation*}
        Since $\{\delta_t \mid t \in \Gamma\}$ is a basis for $\bfF(\Gamma)$, every $f \in \bfF(\Gamma)$ looks like:
            \begin{equation*}
            \begin{split}
                f = \sum_{t \in J}\alpha_t \delta_t , \hspace{4pt}J \subseteq T \hspace{4pt}\text{finite}.
            \end{split}
            \end{equation*}
        Using distributivity we get:
            \begin{equation*}
            \begin{split}
                \delta_r \ast (\delta_s \ast \delta_t)
                & = \delta_r \ast \delta_{st} \\
                & = \delta_{rst} \\
                & = \delta_{rs} \ast \delta_t \\
                & = (\delta_r \ast \delta_s) \ast \delta_t.
            \end{split}
            \end{equation*}
        Whence convolution is associative.
    \end{example}

    \begin{center}
        \begin{tikzpicture}
            \draw[thick] (0.3,0) -- (2.3,0);
            \node at (2.39, 0) {$/\,$};
            \node at (2.56, 0) {$/\,$};
            \draw[thick] (2.6,0) -- (4.6,0);
        \end{tikzpicture}
    \end{center}

    \begin{proposition}*
        Let $\{A_i\}_{i \in I}$ be a family of algebras over $F$.
        \begin{enumerate}[label = (\arabic*),itemsep=1pt,topsep=3pt]
            \item $\prod_{i \in I}A_i$ is an algebra under $(a_i)_i(b_i)_i = (a_ib_i)_i$.
            \item $\bigoplus_{i \in I} \subseteq \prod_{i \in I}A_i$ is an ideal.
        \end{enumerate}
    \end{proposition}
        \begin{proof}
            
        \end{proof}

    \begin{proposition}*
        Let $A$ be an algebra over $F$ and $I \subseteq A$ an ideal. Then $A/I$ is an algebra under $(a+I)(b+I) = ab+I$.
    \end{proposition}
        \begin{proof}
            
        \end{proof}



    
    
